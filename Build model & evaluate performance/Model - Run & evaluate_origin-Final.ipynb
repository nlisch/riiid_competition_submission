{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import math\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import collections\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import feather\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score,precision_recall_curve\n",
    "from sklearn.metrics import make_scorer,f1_score, recall_score, accuracy_score, average_precision_score, auc\n",
    "from sklearn.metrics import f1_score, mean_squared_error, r2_score, classification_report, confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "import matplotlib.style as style\n",
    "style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import datatable as dt\n",
    "from datatable import (dt, f, g, by, ifelse, update, sort, join,\n",
    "                      count, min, max, mean, sum, rowsum)\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS = 99271101\n",
    "\n",
    "def get_index_np():\n",
    "    return np.arange(N_ROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_np(file_path, columns_selected):\n",
    "    data = np.load(file_path , allow_pickle=True)\n",
    "    df = pd.DataFrame(index=get_index_np())\n",
    "    for col in tqdm(columns_selected):\n",
    "        df[col] = data[col]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_np_multifiles(file_path, file_path2, file_path3, columns_selected, columns_selected2, columns_selected3):\n",
    "    data = np.load(file_path , allow_pickle=True)\n",
    "    data2 = np.load(file_path2 , allow_pickle=True)\n",
    "    data3 = np.load(file_path3 , allow_pickle=True)\n",
    "    df = pd.DataFrame(index=get_index_np())\n",
    "    for col in tqdm(columns_selected):\n",
    "        df[col] = data[col]\n",
    "    for col in tqdm(columns_selected2):\n",
    "        df[col] = data2[col]\n",
    "    for col in tqdm(columns_selected3):\n",
    "        df[col] = data3[col]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Check features data\n",
    "file_path = '/Users/nlisch/downloads/Kaggle_riid/process/data_features_final_new.npz'\n",
    "index= ['user_id']\n",
    "\n",
    "index_data = load_data_np(file_path, index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_train_val_idxs(TRAIN_SIZE, VAL_SIZE):\n",
    "    train_idxs = []\n",
    "    val_idxs = []\n",
    "    NEW_USER_FRAC = 1/4 # fraction of new users in \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # create df with user_ids and indices\n",
    "    df = pd.DataFrame(index=get_index_np())\n",
    "    for col in ['user_id']:\n",
    "        df[col] = index_data[col]\n",
    "\n",
    "    df['index'] = df.index.values.astype(np.uint32)\n",
    "    user_id_index = df.groupby('user_id')['index'].apply(np.array)\n",
    "    \n",
    "    # iterate over users in random order\n",
    "    for indices in user_id_index.sample(user_id_index.size, random_state=42):\n",
    "        if len(train_idxs) > TRAIN_SIZE:\n",
    "            break\n",
    "\n",
    "        # fill validation data\n",
    "        if len(val_idxs) < VAL_SIZE:\n",
    "            # add new user\n",
    "            if np.random.rand() < NEW_USER_FRAC:\n",
    "                val_idxs += list(indices)\n",
    "            # randomly split user between train and val otherwise\n",
    "            else:\n",
    "                offset = np.random.randint(0, indices.size)\n",
    "                train_idxs += list(indices[:offset])\n",
    "                val_idxs += list(indices[offset:])\n",
    "        else:\n",
    "            train_idxs += list(indices)\n",
    "        \n",
    "    return train_idxs, val_idxs\n",
    "train_idxs, val_idxs = get_train_val_idxs(int(100e6), 5e6)\n",
    "#train_idxs, val_idxs = get_train_val_idxs(int(50e6), 2.5e6)\n",
    "#train_idxs, val_idxs = get_train_val_idxs(int(10e6), 0.5e6)\n",
    "print(f'len train_idxs: {len(train_idxs)}, len validation_idxs: {len(val_idxs)}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def load_data_np_multifiles(train_idxs, val_idxs, file_path, file_path2, file_path3, columns_selected, columns_selected2, columns_selected3):\n",
    "    data = np.load(file_path , allow_pickle=True)\n",
    "    data2 = np.load(file_path2 , allow_pickle=True)\n",
    "    data3 = np.load(file_path3 , allow_pickle=True)\n",
    "    train = pd.DataFrame(index=train_idxs)\n",
    "    test = pd.DataFrame(index=val_idxs)\n",
    "    for col in tqdm(columns_selected):\n",
    "        train[col] = data[col][train_idxs]\n",
    "        test[col] = data[col][val_idxs]\n",
    "        \n",
    "    for col in tqdm(columns_selected2):\n",
    "        train[col] = data2[col][train_idxs]\n",
    "        test[col] = data2[col][val_idxs]\n",
    "        \n",
    "    for col in tqdm(columns_selected3):\n",
    "        train[col] = data3[col][train_idxs]\n",
    "        test[col] = data3[col][val_idxs]\n",
    "        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_np_multifiles(train_idxs,\n",
    "                            val_idxs,\n",
    "                            file_path,\n",
    "                            file_path2,\n",
    "                            file_path3,\n",
    "                            file_path4,\n",
    "                            file_path5,\n",
    "                            file_path6,\n",
    "                            file_path7,\n",
    "                            file_path8,\n",
    "                            file_path9,\n",
    "                            file_path10,\n",
    "                            columns_selected,\n",
    "                            columns_selected2,\n",
    "                            columns_selected3,\n",
    "                            columns_selected4,\n",
    "                            columns_selected5,\n",
    "                            columns_selected6,\n",
    "                            columns_selected7,\n",
    "                            columns_selected8,\n",
    "                            columns_selected9,\n",
    "                            columns_selected10):\n",
    "    \n",
    "    data = np.load(file_path , allow_pickle=True)\n",
    "    data2 = np.load(file_path2 , allow_pickle=True)\n",
    "    data3 = np.load(file_path3 , allow_pickle=True)\n",
    "    data4 = np.load(file_path4 , allow_pickle=True)\n",
    "    data5 = np.load(file_path5 , allow_pickle=True)\n",
    "    data6 = np.load(file_path6 , allow_pickle=True)\n",
    "    data7 = np.load(file_path7 , allow_pickle=True)\n",
    "    data8 = np.load(file_path8 , allow_pickle=True)\n",
    "    data9 = np.load(file_path9 , allow_pickle=True)\n",
    "    data10 = np.load(file_path10 , allow_pickle=True)\n",
    "    #data11 = np.load(file_path11 , allow_pickle=True)\n",
    "    \n",
    "    X_train = np.ndarray(shape=(len(train_idxs), len(global_features)), dtype=np.float32)\n",
    "    X_val = np.ndarray(shape=(len(val_idxs), len(global_features)), dtype=np.float32)\n",
    "    for col in tqdm(columns_selected):\n",
    "        X_train[:,global_features_dict.get(col)] = data[col][train_idxs].astype(np.float32)\n",
    "        X_val[:,global_features_dict.get(col)] = data[col][val_idxs].astype(np.float32)\n",
    "        \n",
    "    for col in tqdm(columns_selected2):\n",
    "        #print(col)\n",
    "        #print(global_features_dict.get(col))\n",
    "        X_train[:,global_features_dict.get(col)] = data2[col][train_idxs].astype(np.int8)\n",
    "        X_val[:,global_features_dict.get(col)] = data2[col][val_idxs].astype(np.int8)\n",
    "        \n",
    "    for col in tqdm(columns_selected3):\n",
    "        X_train[:,global_features_dict.get(col)] = data3[col][train_idxs].astype(np.int8)\n",
    "        X_val[:,global_features_dict.get(col)] = data3[col][val_idxs].astype(np.int8)\n",
    "        \n",
    "    for col in tqdm(columns_selected4):\n",
    "        X_train[:,global_features_dict.get(col)] = data4[col][train_idxs].astype(np.int8)\n",
    "        X_val[:,global_features_dict.get(col)] = data4[col][val_idxs].astype(np.int8)\n",
    "        \n",
    "    for col in tqdm(columns_selected5):\n",
    "        X_train[:,global_features_dict.get(col)] = data5[col][train_idxs].astype(np.int8)\n",
    "        X_val[:,global_features_dict.get(col)] = data5[col][val_idxs].astype(np.int8)\n",
    "        \n",
    "    for col in tqdm(columns_selected6):\n",
    "        X_train[:,global_features_dict.get(col)] = data6[col][train_idxs].astype(np.int8)\n",
    "        X_val[:,global_features_dict.get(col)] = data6[col][val_idxs].astype(np.int8)\n",
    "        \n",
    "    for col in tqdm(columns_selected7):\n",
    "        X_train[:,global_features_dict.get(col)] = data7[col][train_idxs].astype(np.int8)\n",
    "        X_val[:,global_features_dict.get(col)] = data7[col][val_idxs].astype(np.int8)\n",
    "        \n",
    "    for col in tqdm(columns_selected8):\n",
    "        X_train[:,global_features_dict.get(col)] = data8[col][train_idxs].astype(np.int8)\n",
    "        X_val[:,global_features_dict.get(col)] = data8[col][val_idxs].astype(np.int8)\n",
    "        \n",
    "    for col in tqdm(columns_selected9):\n",
    "        X_train[:,global_features_dict.get(col)] = data9[col][train_idxs].astype(np.int8)\n",
    "        X_val[:,global_features_dict.get(col)] = data9[col][val_idxs].astype(np.int8)\n",
    "        \n",
    "    for col in tqdm(columns_selected10):\n",
    "        X_train[:,global_features_dict.get(col)] = data10[col][train_idxs].astype(np.int8)\n",
    "        X_val[:,global_features_dict.get(col)] = data10[col][val_idxs].astype(np.int8)\n",
    "    \n",
    "        # add the target\n",
    "    y_train = data[target][train_idxs].astype(np.int8)\n",
    "    y_val = data[target][val_idxs].astype(np.int8)\n",
    "        \n",
    "    print(f'X_train.shape: {X_train.shape}\\t y_train.shape: {y_train.shape}')\n",
    "    print(f'X_val.shape: {X_val.shape}\\t y_val.shape: {y_val.shape}')\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Check features data\n",
    "file_path = '/Users/nlisch/downloads/Kaggle_riid/process/data_features_final_new.npz'\n",
    "file_path2 = '/Users/nlisch/downloads/Kaggle_riid/process/prediction/questions_columns/question_historic_perf_attempt_full_1.npz'\n",
    "file_path3 = '/Users/nlisch/downloads/Kaggle_riid/process/prediction/questions_columns/question_historic_perf_attempt_full_2.npz'\n",
    "file_path4 = '/Users/nlisch/downloads/Kaggle_riid/process/prediction/questions_columns/question_historic_perf_attempt_full_3.npz'\n",
    "file_path5 = '/Users/nlisch/downloads/Kaggle_riid/process/prediction/questions_columns/question_historic_perf_attempt_full_4.npz'\n",
    "file_path6 = '/Users/nlisch/downloads/Kaggle_riid/process/prediction/questions_columns/question_historic_perf_attempt_full_5.npz'\n",
    "file_path7 = '/Users/nlisch/downloads/Kaggle_riid/process/prediction/questions_columns/question_historic_perf_attempt_full_6.npz'\n",
    "file_path8 = '/Users/nlisch/downloads/Kaggle_riid/process/prediction/questions_columns/question_historic_perf_attempt_full_7.npz'\n",
    "file_path9 = '/Users/nlisch/downloads/Kaggle_riid/process/prediction/questions_columns/question_historic_perf_attempt_full_8.npz'\n",
    "file_path10 = '/Users/nlisch/downloads/Kaggle_riid/process/prediction/questions_columns/question_historic_perf_attempt_full_9.npz'\n",
    "#file_path11 = '/Users/nlisch/downloads/Kaggle_riid/process/prediction/questions_columns/question_historic_perf_attempt_full_10.npz'\n",
    "\n",
    "\n",
    "features = [\n",
    "#'timestamp',\n",
    "#'user_id',\n",
    "'group',\n",
    "'group2',\n",
    "'content_id',\n",
    "##'task_container_id',\n",
    "'prior_question_elapsed_time',\n",
    "##'bundle_id',\n",
    "'part',\n",
    "'previous_false_answer',\n",
    "'content_answered_correctly_mean',\n",
    "#'bundle_answered_correctly_mean',\n",
    "'user_answered_correctly_mean',\n",
    "'user_answered_correctly_count',\n",
    "'hmean_user_content_accuracy',\n",
    "#'content_answered_correctly_count',\n",
    "#'answer_answered_correctly_mean',\n",
    "##'question_answered_progress',\n",
    "#'question_answered_progress_mean',\n",
    "'answered_correctly_user',\n",
    "#'lagtime_stats'\n",
    "           ]\n",
    "\n",
    "\n",
    "features2 = [\n",
    "\n",
    "]\n",
    "\n",
    "features3 = [\n",
    "\n",
    "]\n",
    "\n",
    "features4 = [\n",
    "\n",
    "]\n",
    "\n",
    "features5 = [\n",
    "\n",
    "]\n",
    "\n",
    "features6 = [\n",
    "\n",
    "]\n",
    "\n",
    "features7 = [\n",
    "\n",
    "]\n",
    "\n",
    "features8 = [\n",
    "\n",
    "]\n",
    "\n",
    "features9 = [\n",
    "\n",
    "]\n",
    "\n",
    "features10 = [\n",
    "\n",
    "]\n",
    "\n",
    "features11 = [\n",
    "\n",
    "]\n",
    "\n",
    "target= 'answered_correctly'\n",
    "\n",
    "index_data= ['user_id']\n",
    "\n",
    "columns = copy.deepcopy(features)\n",
    "columns.append(target)\n",
    "\n",
    "\n",
    "#Build features list\n",
    "global_features = features + features2 + features3 + features4 + features5 + features6 + features7 + features8 + features9 + features10 \n",
    "print(len(global_features))\n",
    "\n",
    "global_features_dict = {k: v for v, k in enumerate(global_features)}\n",
    "\n",
    "X_train, X_val, y_train, y_val = load_data_np_multifiles(train_idxs,\n",
    "                    val_idxs,\n",
    "                    file_path,\n",
    "                    file_path2,\n",
    "                    file_path3,\n",
    "                    file_path4,\n",
    "                    file_path5,\n",
    "                    file_path6,\n",
    "                    file_path7,\n",
    "                    file_path8,\n",
    "                    file_path9,\n",
    "                    file_path10,              \n",
    "                    features,\n",
    "                    features2, \n",
    "                    features3,\n",
    "                    features4, \n",
    "                    features5,\n",
    "                    features6, \n",
    "                    features7,\n",
    "                    features8,\n",
    "                    features9,\n",
    "                    features10\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import joblib\n",
    "# save model\n",
    "joblib.dump(X_train, 'X_train.pkl')\n",
    "joblib.dump(X_val, 'X_val.pkl')\n",
    "joblib.dump(y_train, 'y_train.pkl')\n",
    "joblib.dump(y_val, 'y_val.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "import joblib\n",
    "# save model\n",
    "X_train = joblib.load( 'X_train.pkl')\n",
    "X_val = joblib.load( 'X_val.pkl')\n",
    "y_train = joblib.load( 'y_train.pkl')\n",
    "y_val = joblib.load( 'y_val.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target= 'answered_correctly'\n",
    "\n",
    "index_data= ['user_id']\n",
    "\n",
    "columns = copy.deepcopy(features)\n",
    "columns.append(target)\n",
    "\n",
    "\n",
    "#Build features list\n",
    "global_features = features + features2 + features3 + features4 + features5 + features6 + features7 + features8 + features9 + features10 \n",
    "print(len(global_features))\n",
    "\n",
    "global_features_dict = {k: v for v, k in enumerate(global_features)}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train['previous_false_answer'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test['previous_false_answer'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature = ['content_id', 'group', 'group2', 'bundle_id', 'part', 'tags', 'tags_label', 'prior_question_had_explanation', 'task_container_id']\n",
    "categorical_feature_idxs = []\n",
    "for v in categorical_feature:\n",
    "    try:\n",
    "        categorical_feature_idxs.append(features.index(v))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature_idxs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "#26 min\n",
    "def make_x_y(train, test, train_idxs, val_idxs):\n",
    "    # create numpy arrays\n",
    "    X_train = np.ndarray(shape=(len(train_idxs), len(features)), dtype=np.float32)\n",
    "    X_val = np.ndarray(shape=(len(val_idxs), len(features)), dtype=np.float32)\n",
    "    \n",
    "    # now fill them up\n",
    "    for idx, feature in enumerate(tqdm(features)):\n",
    "        X_train[:,idx] = train[feature].astype(np.float32)\n",
    "        X_val[:,idx] = test[feature].astype(np.float32)\n",
    "    \n",
    "    # add the target\n",
    "    y_train = train[target][train_idxs].astype(np.int8)\n",
    "    y_val = test[target][val_idxs].astype(np.int8)\n",
    "                         \n",
    "    return X_train, y_train, X_val, y_val\n",
    "    \n",
    "X_train, y_train, X_val, y_val = make_x_y(train, test,  train_idxs, val_idxs)\n",
    "print(f'X_train.shape: {X_train.shape}\\t y_train.shape: {y_train.shape}')\n",
    "print(f'X_val.shape: {X_val.shape}\\t y_val.shape: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "pd.DataFrame(X_train[:10], columns=features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "pd.DataFrame(X_val[:10], columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train and validation dataset\n",
    "train_data = lgb.Dataset(\n",
    "    data = X_train,\n",
    "    label = y_train,\n",
    "    categorical_feature = None,\n",
    ")\n",
    "\n",
    "val_data = lgb.Dataset(\n",
    "    data = X_val,\n",
    "    label = y_val,\n",
    "    categorical_feature = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del X_train, y_train, X_val, y_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW from: \n",
    "lgbm_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': ['auc'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#13min\n",
    "def train():\n",
    "    evals_result = {}\n",
    "    model = lgb.train(\n",
    "        params = lgbm_params,\n",
    "        train_set = train_data,\n",
    "        valid_sets = [train_data, val_data],\n",
    "        num_boost_round = 100,\n",
    "        verbose_eval = 10,\n",
    "        evals_result = evals_result,\n",
    "        early_stopping_rounds = 10,\n",
    "        categorical_feature = categorical_feature_idxs,\n",
    "        feature_name = global_features\n",
    "    )\n",
    "\n",
    "    # save model\n",
    "    model.save_model(f'/Users/nlisch/downloads/Kaggle_riid/process/prediction/model_final.lgb')\n",
    "    import joblib\n",
    "    # save model\n",
    "    joblib.dump(model, 'lgb_model_final.pkl')\n",
    "    return model, evals_result\n",
    "    \n",
    "model, evals_result = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(evals_result):\n",
    "    for metric in ['auc']:\n",
    "        plt.figure(figsize=(20,8))\n",
    "        \n",
    "        for key in evals_result.keys():\n",
    "            history_len = len(evals_result.get(key)[metric])\n",
    "            history = evals_result.get(key)[metric]\n",
    "            x_axis = np.arange(1, history_len + 1)\n",
    "            plt.plot(x_axis, history, label=key)\n",
    "        \n",
    "        x_ticks = list(filter(lambda e: (e % (history_len // 100 * 10) == 0) or e == 1, x_axis))\n",
    "        plt.xticks(x_ticks, fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "\n",
    "        plt.title(f'{metric.upper()} History of training', fontsize=18);\n",
    "        plt.xlabel('EPOCH', fontsize=16)\n",
    "        plt.ylabel(metric.upper(), fontsize=16)\n",
    "        \n",
    "        if metric in ['auc']:\n",
    "            plt.legend(loc='upper left', fontsize=14)\n",
    "        else:\n",
    "            plt.legend(loc='upper right', fontsize=14)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "plot_history(evals_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature importance in terms of gain and split\n",
    "def show_feature_importances(model, importance_type, max_num_features=10**10):\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances['feature'] = global_features\n",
    "    feature_importances['value'] = pd.DataFrame(model.feature_importance(importance_type))\n",
    "    feature_importances = feature_importances.sort_values(by='value', ascending=False) # sort feature importance\n",
    "    feature_importances.to_csv(f'feature_importances_{importance_type}.csv') # write feature importance to csv\n",
    "    feature_importances = feature_importances[:50] # only show max_num_features\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.xlim([0, feature_importances.value.max()*1.1])\n",
    "    plt.title(f'Feature {importance_type}', fontsize=18);\n",
    "    sns.barplot(data=feature_importances, x='value', y='feature', palette='rocket');\n",
    "    for idx, v in enumerate(feature_importances.value):\n",
    "        plt.text(v, idx, \"  {:.2e}\".format(v))\n",
    "\n",
    "show_feature_importances(model, 'gain')\n",
    "show_feature_importances(model, 'split')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Get potential parameters for lightgbm\n",
    "estimator = lgb.LGBMClassifier()\n",
    "estimator.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check AUC\n",
    "test = pd.DataFrame(X_val, columns=global_features)\n",
    "y_real = y_val\n",
    "y_proba = model.predict(test)\n",
    "test['y_proba'] = y_proba\n",
    "test['y_real'] = y_real\n",
    "print('LGB score: ', roc_auc_score(test['y_real'], test['y_proba']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test.loc[(test['timestamp'] == 0) & (test['content_id'] == 7900), 'y_proba'] = 0.823816\n",
    "test.loc[(test['timestamp'] == 0) & (test['content_id'] == 128), 'y_proba'] = 0.960318\n",
    "test.loc[(test['timestamp'] == 0) & (test['content_id'] == 5692), 'y_proba'] = 0.709006\n",
    "print('LGB score with replacement T0 : ', roc_auc_score(test['y_real'], test['y_proba']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[(test['timestamp'] == 0) & (test['content_id'] == 7900)][['timestamp', 'content_id',\n",
    "       'prior_question_elapsed_time', 'part', 'previous_false_answer',\n",
    "       'content_answered_correctly_mean', 'user_answered_correctly_mean', 'y_proba', 'y_real']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom = test.groupby(['part']).agg({'y_real': 'sum', 'y_proba':'sum' }).reset_index()\n",
    "zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom['auc'] = roc_auc_score(test['y_real'], test['y_proba'])\n",
    "zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# save model\n",
    "joblib.dump(model, 'lgb_model.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_df = pd.DataFrame(X_train, columns=global_features)\n",
    "train_df['answered_correctly'] = y_train.values\n",
    "test_df = pd.DataFrame(X_val, columns=global_features)\n",
    "test_df['answered_correctly'] = y_val.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "from featexp import get_univariate_plots\n",
    "get_univariate_plots(data=train_df, target_col=target, data_test=test_df, features_list=features)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from featexp import get_trend_stats\n",
    "stats = get_trend_stats(data=train_df, target_col=target, data_test=test_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE TEST AND TRAIN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "selected_features = selector.fit_transform(pd.DataFrame(X_train, columns=features), y_train.values)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "selected_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(X_train, columns=global_features)\n",
    "train[target] = y_train\n",
    "test = pd.DataFrame(X_val, columns=global_features)\n",
    "test[target] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#SAVE DATA\n",
    "#Convert pandas columns to array\n",
    "user_id = np.array(train['user_id'].tolist())\n",
    "content_id = np.array(train['content_id'].tolist())\n",
    "#task_container_id = np.array(train['task_container_id'].tolist())\n",
    "answered_correctly = np.array(train['answered_correctly'].tolist())\n",
    "prior_question_elapsed_time = np.array(train['prior_question_elapsed_time'].tolist())\n",
    "#bundle_id = np.array(train['bundle_id'].tolist())\n",
    "part = np.array(train['part'].tolist())\n",
    "previous_false_answer = np.array(train['previous_false_answer'].tolist())\n",
    "previous_correct_answer = np.array(train['previous_correct_answer'].tolist())\n",
    "#content_answered_correctly_skew = np.array(train['content_answered_correctly_skew'].tolist())\n",
    "content_answered_correctly_mean = np.array(train['content_answered_correctly_mean'].tolist())\n",
    "#content_answered_correctly_std = np.array(train['content_answered_correctly_std'].tolist())\n",
    "#user_answered_correctly_skew = np.array(train['user_answered_correctly_skew'].tolist())\n",
    "user_answered_correctly_mean = np.array(train['user_answered_correctly_mean'].tolist())\n",
    "#user_answered_correctly_std = np.array(train['user_answered_correctly_std'].tolist())\n",
    "hmean_user_content_accuracy = np.array(train['hmean_user_content_accuracy'].tolist())\n",
    "\n",
    "#hmean_answer_content_accuracy = np.array(train['hmean_answer_content_accuracy'].tolist())\n",
    "#answer_answered_correctly_mean = np.array(train['answer_answered_correctly_mean'].tolist())\n",
    "#content_answered_correctly_count = np.array(train['content_answered_correctly_count'].tolist())\n",
    "user_answered_correctly_count = np.array(train['user_answered_correctly_count'].tolist())\n",
    "\n",
    "answered_correctly_user = np.array(train['answered_correctly_user'].tolist())\n",
    "\n",
    "question_6878 = np.array(train['question_6878'].tolist())\n",
    "question_3731 = np.array(train['question_3731'].tolist())\n",
    "question_4995 = np.array(train['question_4995'].tolist())\n",
    "question_4414 = np.array(train['question_4414'].tolist())\n",
    "question_1315 = np.array(train['question_1315'].tolist())\n",
    "#question_6908 = np.array(train['question_6908'].tolist())\n",
    "#question_6909 = np.array(train['question_6909'].tolist())\n",
    "#question_6911 = np.array(train['question_6911'].tolist())\n",
    "#question_6910 = np.array(train['question_6910'].tolist())\n",
    "#question_877 = np.array(train['question_877'].tolist())\n",
    "#question_3717 = np.array(train['question_3717'].tolist())\n",
    "#question_4476 = np.array(train['question_4476'].tolist())\n",
    "question_853 = np.array(train['question_853'].tolist())\n",
    "#question_4108 = np.array(train['question_4108'].tolist())\n",
    "question_951 = np.array(train['question_951'].tolist())\n",
    "question_5177 = np.array(train['question_5177'].tolist())\n",
    "#question_6318 = np.array(train['question_6318'].tolist())\n",
    "#question_199 = np.array(train['question_199'].tolist())\n",
    "#question_7898 = np.array(train['question_7898'].tolist())\n",
    "#question_738 = np.array(train['question_738'].tolist())\n",
    "#question_2594 = np.array(train['question_2594'].tolist())\n",
    "#question_2595 = np.array(train['question_2595'].tolist())\n",
    "question_2593 = np.array(train['question_2593'].tolist())\n",
    "question_7218 = np.array(train['question_7218'].tolist())\n",
    "question_7217 = np.array(train['question_7217'].tolist())\n",
    "question_7216 = np.array(train['question_7216'].tolist())\n",
    "#question_7219 = np.array(train['question_7219'].tolist())\n",
    "question_10686 = np.array(train['question_10686'].tolist())\n",
    "question_294 = np.array(train['question_294'].tolist())\n",
    "#question_10685 = np.array(train['question_10685'].tolist())\n",
    "#question_10688 = np.array(train['question_10688'].tolist())\n",
    "question_10684 = np.array(train['question_10684'].tolist())\n",
    "#question_10687 = np.array(train['question_10687'].tolist())\n",
    "#question_3878 = np.array(train['question_3878'].tolist())\n",
    "#question_6879 = np.array(train['question_6879'].tolist())\n",
    "question_6877 = np.array(train['question_6877'].tolist())\n",
    "#question_6880 = np.array(train['question_6880'].tolist())\n",
    "question_6116 = np.array(train['question_6116'].tolist())\n",
    "question_6173 = np.array(train['question_6173'].tolist())\n",
    "question_4120 = np.array(train['question_4120'].tolist())\n",
    "#question_175 = np.array(train['question_175'].tolist())\n",
    "question_7876 = np.array(train['question_7876'].tolist())\n",
    "question_7900 = np.array(train['question_7900'].tolist())\n",
    "question_2065 = np.array(train['question_2065'].tolist())\n",
    "question_2064 = np.array(train['question_2064'].tolist())\n",
    "#question_2063 = np.array(train['question_2063'].tolist())\n",
    "question_4492 = np.array(train['question_4492'].tolist())\n",
    "#question_3365 = np.array(train['question_3365'].tolist())\n",
    "question_3364 = np.array(train['question_3364'].tolist())\n",
    "#question_3363 = np.array(train['question_3363'].tolist())\n",
    "#question_4696 = np.array(train['question_4696'].tolist())\n",
    "#question_1278 = np.array(train['question_1278'].tolist())\n",
    "question_6370 = np.array(train['question_6370'].tolist())\n",
    "#question_2946 = np.array(train['question_2946'].tolist())\n",
    "#question_2947 = np.array(train['question_2947'].tolist())\n",
    "question_answered_progress = np.array(train['question_answered_progress'].tolist()) \n",
    "question_answered_progress_mean = np.array(train['question_answered_progress_mean'].tolist())\n",
    "#hmean_user_content_accuracy_progress_question = np.array(train['hmean_user_content_accuracy_progress_question'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.savez_compressed('/Users/nlisch/downloads/Kaggle_riid/process/prediction/train_model_f2.npz',\n",
    "                    #From data file\n",
    "                    user_id = user_id,\n",
    "                    content_id = content_id,\n",
    "                    #task_container_id = task_container_id,\n",
    "                    answered_correctly = answered_correctly,\n",
    "                    prior_question_elapsed_time = prior_question_elapsed_time,\n",
    "                    #bundle_id = bundle_id,\n",
    "                    part = part,\n",
    "                    previous_false_answer = previous_false_answer,\n",
    "                    previous_correct_answer = previous_correct_answer,\n",
    "                    #content_answered_correctly_skew = content_answered_correctly_skew,\n",
    "                    content_answered_correctly_mean = content_answered_correctly_mean,\n",
    "                    #content_answered_correctly_std = content_answered_correctly_std,\n",
    "                    #user_answered_correctly_skew = user_answered_correctly_skew,\n",
    "                    user_answered_correctly_mean = user_answered_correctly_mean,\n",
    "                    #user_answered_correctly_std = user_answered_correctly_std,\n",
    "                    hmean_user_content_accuracy = hmean_user_content_accuracy,\n",
    "                    #hmean_answer_content_accuracy = hmean_answer_content_accuracy,\n",
    "                    #answer_answered_correctly_mean = answer_answered_correctly_mean,\n",
    "                    #content_answered_correctly_count = content_answered_correctly_count, \n",
    "                    user_answered_correctly_count = user_answered_correctly_count,\n",
    "                    answered_correctly_user = answered_correctly_user,\n",
    "                    \n",
    "                    question_6878 = question_6878,\n",
    "                    question_3731 = question_3731,\n",
    "                    question_4995 = question_4995,\n",
    "                    question_4414 = question_4414,\n",
    "                    question_1315 = question_1315,\n",
    "                    #question_6908 = question_6908,\n",
    "                    #question_6909 = question_6909,\n",
    "                    #question_6911 = question_6911,\n",
    "                    #question_6910 = question_6910,\n",
    "                    #question_877 = question_877,\n",
    "                    #question_3717 = question_3717,\n",
    "                    #question_4476 = question_4476,\n",
    "                    question_853 = question_853,\n",
    "                    #question_4108 = question_4108,\n",
    "                    question_951 = question_951,\n",
    "                    question_5177 = question_5177,\n",
    "                    #question_6318 = question_6318,\n",
    "                    #question_199 = question_199,\n",
    "                    #question_7898 = question_7898,\n",
    "                    #question_738 = question_738,\n",
    "                    #question_2594 = question_2594,\n",
    "                    #question_2595 = question_2595,\n",
    "                    question_2593 = question_2593,\n",
    "                    question_7218 = question_7218,\n",
    "                    question_7217 = question_7217,\n",
    "                    question_7216 = question_7216,\n",
    "                    #question_7219 = question_7219,\n",
    "                    question_10686 = question_10686,\n",
    "                    question_294 = question_294,\n",
    "                    #question_10685 = question_10685,\n",
    "                    #question_10688 = question_10688,\n",
    "                    question_10684 = question_10684,\n",
    "                    #question_10687 = question_10687,\n",
    "                    #question_3878 = question_3878,\n",
    "                    #question_6879 = question_6879,\n",
    "                    question_6877 = question_6877,\n",
    "                    #question_6880 = question_6880,\n",
    "                    question_6116 = question_6116,\n",
    "                    question_6173 = question_6173,\n",
    "                    question_4120 = question_4120,\n",
    "                    #question_175 = question_175,\n",
    "                    question_7876 = question_7876,\n",
    "                    question_7900 = question_7900,\n",
    "                    question_2065 = question_2065,\n",
    "                    question_2064 = question_2064,\n",
    "                    #question_2063 = question_2063,\n",
    "                    question_4492 = question_4492,\n",
    "                    #question_3365 = question_3365,\n",
    "                    question_3364 = question_3364,\n",
    "                    #question_3363 = question_3363,\n",
    "                    #question_4696 = question_4696,\n",
    "                    #question_1278 = question_1278,\n",
    "                    question_6370 = question_6370,\n",
    "                    #question_2946 = question_2946,\n",
    "                    #question_2947 = question_2947,\n",
    "                    question_answered_progress = question_answered_progress ,\n",
    "                    question_answered_progress_mean = question_answered_progress_mean,\n",
    "                    #hmean_user_content_accuracy_progress_question = hmean_user_content_accuracy_progress_question\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#SAVE DATA\n",
    "#Convert pandas columns to array\n",
    "user_id = np.array(test['user_id'].tolist())\n",
    "content_id = np.array(test['content_id'].tolist())\n",
    "#task_container_id = np.array(test['task_container_id'].tolist())\n",
    "answered_correctly = np.array(test['answered_correctly'].tolist())\n",
    "prior_question_elapsed_time = np.array(test['prior_question_elapsed_time'].tolist())\n",
    "#bundle_id = np.array(test['bundle_id'].tolist())\n",
    "part = np.array(test['part'].tolist())\n",
    "previous_false_answer = np.array(test['previous_false_answer'].tolist())\n",
    "previous_correct_answer = np.array(test['previous_correct_answer'].tolist())\n",
    "#content_answered_correctly_skew = np.array(test['content_answered_correctly_skew'].tolist())\n",
    "content_answered_correctly_mean = np.array(test['content_answered_correctly_mean'].tolist())\n",
    "#content_answered_correctly_std = np.array(test['content_answered_correctly_std'].tolist())\n",
    "#user_answered_correctly_skew = np.array(test['user_answered_correctly_skew'].tolist())\n",
    "user_answered_correctly_mean = np.array(test['user_answered_correctly_mean'].tolist())\n",
    "#user_answered_correctly_std = np.array(test['user_answered_correctly_std'].tolist())\n",
    "hmean_user_content_accuracy = np.array(test['hmean_user_content_accuracy'].tolist())\n",
    "\n",
    "#hmean_answer_content_accuracy = np.array(test['hmean_answer_content_accuracy'].tolist())\n",
    "#answer_answered_correctly_mean = np.array(test['answer_answered_correctly_mean'].tolist())\n",
    "#content_answered_correctly_count = np.array(test['content_answered_correctly_count'].tolist())\n",
    "user_answered_correctly_count = np.array(test['user_answered_correctly_count'].tolist())\n",
    "\n",
    "answered_correctly_user = np.array(test['answered_correctly_user'].tolist())\n",
    "\n",
    "question_6878 = np.array(test['question_6878'].tolist())\n",
    "question_3731 = np.array(test['question_3731'].tolist())\n",
    "question_4995 = np.array(test['question_4995'].tolist())\n",
    "question_4414 = np.array(test['question_4414'].tolist())\n",
    "question_1315 = np.array(test['question_1315'].tolist())\n",
    "#question_6908 = np.array(test['question_6908'].tolist())\n",
    "#question_6909 = np.array(test['question_6909'].tolist())\n",
    "#question_6911 = np.array(test['question_6911'].tolist())\n",
    "#question_6910 = np.array(test['question_6910'].tolist())\n",
    "#question_877 = np.array(test['question_877'].tolist())\n",
    "#question_3717 = np.array(test['question_3717'].tolist())\n",
    "#question_4476 = np.array(test['question_4476'].tolist())\n",
    "question_853 = np.array(test['question_853'].tolist())\n",
    "#question_4108 = np.array(test['question_4108'].tolist())\n",
    "question_951 = np.array(test['question_951'].tolist())\n",
    "question_5177 = np.array(test['question_5177'].tolist())\n",
    "#question_6318 = np.array(test['question_6318'].tolist())\n",
    "#question_199 = np.array(test['question_199'].tolist())\n",
    "#question_7898 = np.array(test['question_7898'].tolist())\n",
    "#question_738 = np.array(test['question_738'].tolist())\n",
    "#question_2594 = np.array(test['question_2594'].tolist())\n",
    "#question_2595 = np.array(test['question_2595'].tolist())\n",
    "question_2593 = np.array(test['question_2593'].tolist())\n",
    "question_7218 = np.array(test['question_7218'].tolist())\n",
    "question_7217 = np.array(test['question_7217'].tolist())\n",
    "question_7216 = np.array(test['question_7216'].tolist())\n",
    "#question_7219 = np.array(test['question_7219'].tolist())\n",
    "question_10686 = np.array(test['question_10686'].tolist())\n",
    "question_294 = np.array(test['question_294'].tolist())\n",
    "#question_10685 = np.array(test['question_10685'].tolist())\n",
    "#question_10688 = np.array(test['question_10688'].tolist())\n",
    "question_10684 = np.array(test['question_10684'].tolist())\n",
    "#question_10687 = np.array(test['question_10687'].tolist())\n",
    "#question_3878 = np.array(test['question_3878'].tolist())\n",
    "#question_6879 = np.array(test['question_6879'].tolist())\n",
    "question_6877 = np.array(test['question_6877'].tolist())\n",
    "#question_6880 = np.array(test['question_6880'].tolist())\n",
    "question_6116 = np.array(test['question_6116'].tolist())\n",
    "question_6173 = np.array(test['question_6173'].tolist())\n",
    "question_4120 = np.array(test['question_4120'].tolist())\n",
    "#question_175 = np.array(test['question_175'].tolist())\n",
    "question_7876 = np.array(test['question_7876'].tolist())\n",
    "question_7900 = np.array(test['question_7900'].tolist())\n",
    "question_2065 = np.array(test['question_2065'].tolist())\n",
    "question_2064 = np.array(test['question_2064'].tolist())\n",
    "#question_2063 = np.array(test['question_2063'].tolist())\n",
    "question_4492 = np.array(test['question_4492'].tolist())\n",
    "#question_3365 = np.array(test['question_3365'].tolist())\n",
    "question_3364 = np.array(test['question_3364'].tolist())\n",
    "#question_3363 = np.array(test['question_3363'].tolist())\n",
    "#question_4696 = np.array(test['question_4696'].tolist())\n",
    "#question_1278 = np.array(test['question_1278'].tolist())\n",
    "question_6370 = np.array(test['question_6370'].tolist())\n",
    "#question_2946 = np.array(test['question_2946'].tolist())\n",
    "#question_2947 = np.array(test['question_2947'].tolist())\n",
    "question_answered_progress = np.array(test['question_answered_progress'].tolist()) \n",
    "question_answered_progress_mean = np.array(test['question_answered_progress_mean'].tolist())\n",
    "#hmean_user_content_accuracy_progress_question = np.array(test['hmean_user_content_accuracy_progress_question'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.savez_compressed('/Users/nlisch/downloads/Kaggle_riid/process/prediction/test_model_f2.npz',\n",
    "                    #From data file\n",
    "                    user_id = user_id,\n",
    "                    content_id = content_id,\n",
    "                    #task_container_id = task_container_id,\n",
    "                    answered_correctly = answered_correctly,\n",
    "                    prior_question_elapsed_time = prior_question_elapsed_time,\n",
    "                    #bundle_id = bundle_id,\n",
    "                    part = part,\n",
    "                    previous_false_answer = previous_false_answer,\n",
    "                    previous_correct_answer = previous_correct_answer,\n",
    "                    #content_answered_correctly_skew = content_answered_correctly_skew,\n",
    "                    content_answered_correctly_mean = content_answered_correctly_mean,\n",
    "                    #content_answered_correctly_std = content_answered_correctly_std,\n",
    "                    #user_answered_correctly_skew = user_answered_correctly_skew,\n",
    "                    user_answered_correctly_mean = user_answered_correctly_mean,\n",
    "                    #user_answered_correctly_std = user_answered_correctly_std,\n",
    "                    hmean_user_content_accuracy = hmean_user_content_accuracy,\n",
    "                    #hmean_answer_content_accuracy = hmean_answer_content_accuracy,\n",
    "                    #answer_answered_correctly_mean = answer_answered_correctly_mean,\n",
    "                    #content_answered_correctly_count = content_answered_correctly_count, \n",
    "                    user_answered_correctly_count = user_answered_correctly_count,\n",
    "                    answered_correctly_user = answered_correctly_user,\n",
    "                    \n",
    "                    question_6878 = question_6878,\n",
    "                    question_3731 = question_3731,\n",
    "                    question_4995 = question_4995,\n",
    "                    question_4414 = question_4414,\n",
    "                    question_1315 = question_1315,\n",
    "                    #question_6908 = question_6908,\n",
    "                    #question_6909 = question_6909,\n",
    "                    #question_6911 = question_6911,\n",
    "                    #question_6910 = question_6910,\n",
    "                    #question_877 = question_877,\n",
    "                    #question_3717 = question_3717,\n",
    "                    #question_4476 = question_4476,\n",
    "                    question_853 = question_853,\n",
    "                    #question_4108 = question_4108,\n",
    "                    question_951 = question_951,\n",
    "                    question_5177 = question_5177,\n",
    "                    #question_6318 = question_6318,\n",
    "                    #question_199 = question_199,\n",
    "                    #question_7898 = question_7898,\n",
    "                    #question_738 = question_738,\n",
    "                    #question_2594 = question_2594,\n",
    "                    #question_2595 = question_2595,\n",
    "                    question_2593 = question_2593,\n",
    "                    question_7218 = question_7218,\n",
    "                    question_7217 = question_7217,\n",
    "                    question_7216 = question_7216,\n",
    "                    #question_7219 = question_7219,\n",
    "                    question_10686 = question_10686,\n",
    "                    question_294 = question_294,\n",
    "                    #question_10685 = question_10685,\n",
    "                    #question_10688 = question_10688,\n",
    "                    question_10684 = question_10684,\n",
    "                    #question_10687 = question_10687,\n",
    "                    #question_3878 = question_3878,\n",
    "                    #question_6879 = question_6879,\n",
    "                    question_6877 = question_6877,\n",
    "                    #question_6880 = question_6880,\n",
    "                    question_6116 = question_6116,\n",
    "                    question_6173 = question_6173,\n",
    "                    question_4120 = question_4120,\n",
    "                    #question_175 = question_175,\n",
    "                    question_7876 = question_7876,\n",
    "                    question_7900 = question_7900,\n",
    "                    question_2065 = question_2065,\n",
    "                    question_2064 = question_2064,\n",
    "                    #question_2063 = question_2063,\n",
    "                    question_4492 = question_4492,\n",
    "                    #question_3365 = question_3365,\n",
    "                    question_3364 = question_3364,\n",
    "                    #question_3363 = question_3363,\n",
    "                    #question_4696 = question_4696,\n",
    "                    #question_1278 = question_1278,\n",
    "                    question_6370 = question_6370,\n",
    "                    #question_2946 = question_2946,\n",
    "                    #question_2947 = question_2947,\n",
    "                    question_answered_progress = question_answered_progress ,\n",
    "                    question_answered_progress_mean = question_answered_progress_mean,\n",
    "                    #hmean_user_content_accuracy_progress_question = hmean_user_content_accuracy_progress_question\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS = 99271101\n",
    "N_ROWS_train = 50004223\n",
    "N_ROWS_test = 2500267\n",
    "N_ROWS_user_stats = 393656\n",
    "N_ROWS_content_stats = 13523\n",
    "N_ROWS_previous_stats_questions = 86867031\n",
    "def get_index_np(type_data):\n",
    "    if type_data == 'train':\n",
    "        final = np.arange(N_ROWS_train)\n",
    "    if type_data == 'test':\n",
    "        final = np.arange(N_ROWS_test)\n",
    "    if type_data == 'user_stats':\n",
    "        final = np.arange(N_ROWS_user_stats)\n",
    "    if type_data == 'content_stats':\n",
    "        final = np.arange(N_ROWS_content_stats)\n",
    "    if type_data == 'previous_stats_questions':\n",
    "        final = np.arange(N_ROWS_previous_stats_questions)\n",
    "    if type_data == 'full_dataset':\n",
    "        final = np.arange(N_ROWS)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_np(file_path, columns_selected, type_data):\n",
    "    data = np.load(file_path , allow_pickle=True)\n",
    "    df = pd.DataFrame(index=get_index_np(type_data))\n",
    "    for col in tqdm(columns_selected):\n",
    "        df[col] = data[col]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Check features data\n",
    "file_path = '/Users/nlisch/downloads/Kaggle_riid/process/prediction/train_model_f.npz'\n",
    "columns = ['answered_correctly_user']\n",
    "\n",
    "data = load_data_np(file_path, columns, 'train' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shappley value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(X_val, columns=global_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(test[global_features])\n",
    "\n",
    "#print('Shap Values shape : {}'.format(shap_values.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Calculate dataframe of features importance following SHAP Values\n",
    "shap_sum = np.abs(shap_values).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "importance_shap_values = pd.DataFrame([test[global_features].columns.tolist(), shap_sum.tolist()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "importance_shap_values.columns = ['column_name', 'shap_importance']\n",
    "importance_shap_values = importance_shap_values.sort_values('shap_importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Plot Features importance following SHAP Values\n",
    "shap.summary_plot(shap_values, test[global_features], plot_type=\"bar\", max_display=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_shap_values.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
